{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentModelAdministrationClient, ClassifierDocumentTypeDetails, BlobSource,BlobFileListSource\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter all the credentials.\n",
    "\n",
    "Note that they will expire soon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://analyzedocuments.cognitiveservices.azure.com/\"\n",
    "key = \"dd508fa001674a76a10c909e3268b98f\"  # Replace with your key\n",
    "container_sas_url = \"https://materiallabstorage.blob.core.windows.net/experimentstorage?sp=racwdli&st=2024-02-01T16:39:25Z&se=2024-02-02T00:39:25Z&sv=2022-11-02&sr=c&sig=hJu0jwegGqds%2BauIwxRgCT6H9LuUvV0vgjL4HoqRTJ0%3D\"  # Replace with your container SAS URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we work on a supervised approach. Devising an unsupervised one is possible. This is still in progress.\n",
    "\n",
    "All the training files are stored in `container_sas_url`, an azure blob storage, in the same directory.\n",
    "\n",
    "For each class, we need AT LEAST 5 samples. Here, we save their filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1_files = [\n",
    "    \"20190703173714952-1.pdf\",\n",
    "    \"20190703173714952-2.pdf\",\n",
    "    \"20190703173714952-3.pdf\",\n",
    "    \"20190703173714952-4.pdf\",\n",
    "    \"20190703173714952-5.pdf\"\n",
    "]\n",
    "\n",
    "class_2_files = [\n",
    "    \"20190703173714952-10.pdf\",\n",
    "    \"20190703173714952-6.pdf\",\n",
    "    \"20190703173714952-7.pdf\",\n",
    "    \"20190703173714952-8.pdf\",\n",
    "    \"20190703173714952-9.pdf\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function trains the classifier using the training files above. It outputs the `classifier_id`, which we need to record for application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_build_classifier():\n",
    "    endpoint = \"https://analyzedocuments.cognitiveservices.azure.com/\"\n",
    "    key = \"dd508fa001674a76a10c909e3268b98f\"  # Replace with your key\n",
    "    container_sas_url = \"https://materiallabstorage.blob.core.windows.net/experimentstorage?sp=racwdli&st=2024-02-01T16:39:25Z&se=2024-02-02T00:39:25Z&sv=2022-11-02&sr=c&sig=hJu0jwegGqds%2BauIwxRgCT6H9LuUvV0vgjL4HoqRTJ0%3D\"  # Replace with your container SAS URL\n",
    "\n",
    "    document_model_admin_client = DocumentModelAdministrationClient(\n",
    "        endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "    )\n",
    "\n",
    "    # Combine the file lists into a JSON structure\n",
    "    filelist_json = json.dumps({\n",
    "        \"class_1_files\": class_1_files,\n",
    "        \"class_2_files\": class_2_files\n",
    "    })\n",
    "    poller = document_model_admin_client.begin_build_document_classifier(\n",
    "        doc_types={\n",
    "            \"class_1\": ClassifierDocumentTypeDetails(\n",
    "                source=BlobFileListSource(\n",
    "                    container_url=container_sas_url, file_list= \"class_1.jsonl\"\n",
    "                    # container_url=container_sas_url, prefix=\"training_data/class_1/\"\n",
    "                )\n",
    "            ),\n",
    "            \"class_2\": ClassifierDocumentTypeDetails(\n",
    "                source=BlobFileListSource(\n",
    "                    container_url=container_sas_url, file_list= \"class_2.jsonl\"\n",
    "                    # container_url=container_sas_url, prefix=\"training_data/class_2/\"\n",
    "                )\n",
    "            ),\n",
    "        },\n",
    "        description=\"document classifier\",\n",
    "        # classifier_id = 'document_classifier'\n",
    "        # model_build_mode=\"template\"  # Use 'neural' for a neural network-based model\n",
    "    )\n",
    "\n",
    "    result = poller.result()\n",
    "    print(f\"Classifier ID: {result.classifier_id}\")\n",
    "    print(f\"API version used to build the classifier model: {result.api_version}\")\n",
    "    print(f\"Classifier description: {result.description}\")\n",
    "    print(f\"Document classes used for training the model:\")\n",
    "    for doc_type, details in result.doc_types.items():\n",
    "        print(f\"Document type: {doc_type}\")\n",
    "        print(f\"Container source: {details.source.container_url}\\n\")\n",
    "    return result.classifier_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to build your classifier\n",
    "classifier_id = sample_build_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function tests our classifier using the data in the folder `testing_data`, locally stored. It prints out the classification results and the confidence scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test_classifier(classifier_id):\n",
    "    from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    import os\n",
    "\n",
    "    # Replace with your Form Recognizer endpoint and key\n",
    "    endpoint = \"https://analyzedocuments.cognitiveservices.azure.com/\"\n",
    "    key = \"dd508fa001674a76a10c909e3268b98f\"\n",
    "\n",
    "    # Create an authenticated client\n",
    "    document_analysis_client = DocumentAnalysisClient(endpoint, AzureKeyCredential(key))\n",
    "\n",
    "    # Path to the testing data folder\n",
    "    folder_path = \"testing_data\"\n",
    "\n",
    "    # Get a list of all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Filter for PDF files if necessary\n",
    "    pdf_files = [file for file in files if file.lower().endswith('.pdf')]\n",
    "\n",
    "    # Iterate over each file in the folder\n",
    "    for file_name in pdf_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Open the file and send it to Form Recognizer for analysis\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            poller = document_analysis_client.begin_classify_document(classifier_id=classifier_id, document=f)\n",
    "            result = poller.result()\n",
    "\n",
    "        # Process the results\n",
    "        print(f\"Results for file: {file_name}\")\n",
    "        for idx, doc in enumerate(result.documents):\n",
    "            print(f\"  Document {idx + 1}: {doc.doc_type}\")\n",
    "            print(f\"  Confidence score: {doc.confidence}\\n\")\n",
    "            for name, field in doc.fields.items():\n",
    "                field_value = field.value if field.value else field.content\n",
    "                print(f\"  Field: {name}, Value: {field_value}, Confidence: {field.confidence}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"document_classifier\"\n",
    "sample_test_classifier(classifier_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Work in Progress:\n",
    "\n",
    "- For test samples with low confidence scores, we can define 'anomaly'.\n",
    "- We can plausibly define unsupervised approach, by pre-training a different model before this pipeline, i.e.\n",
    "  \n",
    "  data -> unsupervised (e.g. clustering) -> classified --> `sample_build_classifier`()\n",
    "  \n",
    "  Then, when 'anomalies' reach a certain number, re-train the model and then re-classify all the documents. \n",
    "\n",
    "- We can create a filing system, directly systemize the files according to the classification results."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
